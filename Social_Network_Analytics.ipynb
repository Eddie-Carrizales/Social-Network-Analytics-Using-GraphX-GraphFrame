{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62755d4e-b0d4-49e8-ae70-6b6c37844156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --Analyzing Social Networks using GraphX/GraphFrame--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99c1f28-6367-43d3-854b-7e76e2e54490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| src|dst|\n",
      "+----+---+\n",
      "|7188|  1|\n",
      "| 430|  1|\n",
      "|3134|  1|\n",
      "|3026|  1|\n",
      "|3010|  1|\n",
      "| 804|  1|\n",
      "| 160|  1|\n",
      "|  95|  1|\n",
      "| 377|  1|\n",
      "| 888|  1|\n",
      "+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "#Bitcoin Alpha trust social network dataset source: https://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html\n",
    "# This dataset is containts source, target, rating, time\n",
    "\n",
    "# Loading edge data from the CSV file into a Spark DataFrame\n",
    "edges_df = spark.read.csv(\"dbfs:/FileStore/soc_sign_bitcoinalpha.csv\", header=False, inferSchema=True) # Loading the data\n",
    "edges_df = edges_df.selectExpr(\"_c0 as src\", \"_c1 as dst\") # Renaming the columns as its required by graphframe later\n",
    "edges_df.show(10) # Displaying the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932b352c-fa85-478d-9d6e-bdd64d9ca6f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "| 804|\n",
      "|3026|\n",
      "|7188|\n",
      "|3010|\n",
      "| 377|\n",
      "| 430|\n",
      "| 160|\n",
      "|  95|\n",
      "| 888|\n",
      "|3134|\n",
      "+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the vertices dataframe by combining the source and target nodes\n",
    "vertices_df = edges_df.selectExpr(\"src as id\").union(edges_df.selectExpr(\"dst as id\")).distinct()\n",
    "vertices_df.show(10)  # Displaying the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4825ef-d2a8-4534-99ab-d71aea9636d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:170: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating a GraphFrame using the vertices and edges DataFrames\n",
    "graph = GraphFrame(vertices_df, edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1343b085-d58c-4ab9-95a8-83022e52b882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: GraphFrame(v:[id: int], e:[src: int, dst: int])"
     ]
    }
   ],
   "source": [
    "# Put the graph in memory for faster access\n",
    "graph.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fb1eb2-169a-4fab-aee6-d9a575afd12d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running Queries:\n",
    "# Run the following queries using the GraphX/GraphFrame API and write your output to a file on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030f6d7b-6a23-416f-b0be-97c3f5fd19e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:149: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|outDegree|\n",
      "+---+---------+\n",
      "|  1|      490|\n",
      "|  8|      259|\n",
      "|  3|      243|\n",
      "|  4|      215|\n",
      "|  7|      212|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a. Find the top 5 nodes with the highest outdegree and find the count of the number of outgoing edges in each\n",
    "\n",
    "# We get the outdegrees from the graph, we sort the nodes with them in descending order and grab top 5 nodes\n",
    "top5_out_degree = graph.outDegrees.sort(desc(\"outDegree\")).limit(5)\n",
    "top5_out_degree.show() # display the top 5 nodes with highest outdegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed839a09-a8ad-4d6b-bb16-3a29a61132e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  1|     398|\n",
      "|  3|     251|\n",
      "|  2|     205|\n",
      "| 11|     203|\n",
      "|  4|     201|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b. Find the top 5 nodes with the highest indegree and find the count of the number of incoming edges in each\n",
    "\n",
    "# We get the indegrees from the graph, we sort the nodes with the indegrees in descending order and grab the top 5 nodes\n",
    "top5_in_degree = graph.inDegrees.sort(desc(\"inDegree\")).limit(5)\n",
    "top5_in_degree.show() # display the top 5 nodes with the highest indegrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1a15e0-c0b1-47dc-90a5-2d962cb9a673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          pagerank|\n",
      "+---+------------------+\n",
      "|  1|  64.0285169308339|\n",
      "|  3|33.939965394258394|\n",
      "|  4|30.478870193560947|\n",
      "|  2|25.205056808837032|\n",
      "|177| 25.12373691609342|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c. Calculate PageRank for each of the nodes and output the top 5 nodes with the highest PageRank values. You are free to define any suitable parameters.\n",
    "\n",
    "# First we run the PageRank algorithm algorithm on the graph with the specific parameters from the class lab\n",
    "page_rank_results = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Then we order in descending order and get the top 5 nodes (these will be the vertices with the highest pagerank values)\n",
    "top5_page_rank = page_rank_results.vertices.orderBy(desc(\"pagerank\")).limit(5)\n",
    "top5_page_rank.show() # display the top 5 nodes with the highest pagerank values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd9d03d-d07b-4111-a538-0145c99c87ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|component|count|\n",
      "+---------+-----+\n",
      "|        1| 3775|\n",
      "|     3228|    2|\n",
      "|     1389|    2|\n",
      "|     5837|    2|\n",
      "|     1870|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d. Run the connected components algorithm on it and find the top 5 components with the largest number of nodes.\n",
    "\n",
    "# First we set the checkpoint directory (this is required for the connected components algorithm, we do this like we did in class)\n",
    "sc.setCheckpointDir(\"/tmp/checkpoints\")\n",
    "\n",
    "# Next, we will get the connected components\n",
    "connected_components = graph.connectedComponents()\n",
    "\n",
    "# Now, we will group the nodes by the component and count the number of nodes in each component\n",
    "grouped_components_count = connected_components.groupBy(\"component\").count()\n",
    "\n",
    "# After that we will sort\n",
    "top5_components = grouped_components_count.orderBy(desc(\"count\")).limit(5)\n",
    "top5_components.show() # display the top 5 components with largest number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcea892b-2279-4a15-9f50-bdfb7e12e592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|count| id|\n",
      "+-----+---+\n",
      "| 1815| 11|\n",
      "| 1628|  2|\n",
      "| 1414|177|\n",
      "| 1336|  3|\n",
      "| 1181|  7|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e. Run the triangle counts algorithm on each of the vertices and output the top 5 vertices with the largest triangle count. In case of ties, you can randomly select the top 5 vertices.\n",
    "\n",
    "# First we run the triangle counts algorithm on the graph\n",
    "triangles_count = graph.triangleCount()\n",
    "\n",
    "# Then we sort in descending order and grab the first 5 (which are the top 5 vertices with largest triangle count)\n",
    "top5_triangles = triangles_count.orderBy(desc(\"count\")).limit(5)\n",
    "top5_triangles.show() # Display the top 5 vertices with the largest triangle count"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment 3 Part 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
